# CodeRunner IDE

A desktop IDE that connects to **local LLMs** (and cloud APIs) to write, debug, and iterate on code — all in one window. Type what you want, the LLM writes it, you press Run & Fix, and it works. No cloud required.

![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Platform](https://img.shields.io/badge/Platform-macOS%20|%20Linux%20|%20Windows-lightgrey.svg)

## Play Games Generated by CodeRunner

These games were generated in **one shot** by local LLMs using CodeRunner:

| Game | Play Now | Model Used |
|------|----------|------------|
| **Space Invaders** | [Play](https://jmrothberg.github.io/Code_Runner/Generated_games/Invaders_Qwen3-Coder_next_BESTEVER.html) | Qwen3-Coder |

---

## Quick Start

### 1. Install

```bash
pip install -r requirements.txt
```

### 2. Set Up API Keys (optional — only for cloud backends)

```bash
cp .env.example .env
# Edit .env with your Anthropic/OpenAI keys
```

### 3. Run

```bash
python CodeRunner_IDE_clean.py
```

### 4. Use It

1. Select a backend and model (top of main window)
2. Switch to **Python Programmer** or **HTML Programmer** mode
3. Type in the chat: *"Write a Space Invaders game in Python using Pygame"*
4. Click **Move to IDE**
5. Press **F6** (Run & Fix)
6. If it crashes, the LLM auto-fixes it. You see a diff. Press **Ctrl+Enter** to Accept.
7. If it runs but something is wrong, type what's wrong in chat and click **LLM Fix**

That's the whole workflow.

---

## Why CodeRunner?

Most AI coding tools require cloud APIs. CodeRunner lets you use **your own local models** — keeping your code private, avoiding API costs, and working offline. It supports 8 backends so you can use whatever hardware you have.

---

## The Three Buttons That Matter

| Button | Location | What It Does |
|--------|----------|--------------|
| **Run & Fix** | IDE toolbar (`F6`) | Runs your code. If it crashes, automatically sends the error to the LLM, gets a fix, and shows you a color-coded diff. |
| **LLM Fix** | Chat + IDE toolbar | Code runs but does the wrong thing? Type what's wrong in the chat box, then click this. LLM reads your whole file and returns only the changed functions. |
| **Run** | IDE toolbar (`F5`) | Just run — no LLM involved. |

The **LLM Fix** button appears in both the main chat window and the IDE toolbar for convenience.

---

## The Workflow

```
1. Ask the LLM to write a game in chat
2. Click "Move to IDE" to put the code in the editor
3. Press F6 (Run & Fix)
4. If it crashes  ->  LLM auto-fixes  ->  you see the diff  ->  Accept or Reject
5. If it runs but something is wrong:
     a. Type what's wrong in the chat box ("the ball goes through the paddle")
     b. Click "LLM Fix"
     c. LLM fixes it  ->  you see the diff  ->  Accept or Reject
6. Repeat until it works
```

**You don't need to read the code or find the bug. That's the LLM's job.**

Just describe what's wrong in plain English. The LLM reads your entire program, figures out which functions are broken, and returns only the changes.

### First Shot vs. Fixing

| Phase | What You Do | What the LLM Returns |
|-------|-------------|---------------------|
| **First shot** | Ask the LLM to write code from scratch | Full program |
| **Fixing** | Describe what's wrong, click "LLM Fix" | Only the changed functions (merged into your code) |

### How Fix Mode Works

When you click **LLM Fix**, CodeRunner sends only **two messages** to the LLM:

1. A **fix-specific system message** ("You are an expert debugger...")
2. The **fix prompt** (your full current code + errors + rules)

The full conversation history is **not** sent. This prevents weak local LLMs from seeing old "write me a game" requests and regenerating the entire program. After the LLM responds, the full history is restored so your chat stays intact.

The fix prompt includes strict rules:
- Say what's wrong in 1-2 sentences
- Put ALL changes in ONE code block as complete functions
- Do NOT return the entire program — maximum 50 lines
- No loose instructions outside the code block

### Partial Merge

When the LLM returns only changed functions, CodeRunner **merges them back** into your full program using `SequenceMatcher` opcodes. Lines not in the LLM's response are kept as-is — they're not deletions, just parts the LLM didn't need to change.

### Accept & Reject

When the LLM proposes changes, two buttons light up in the IDE toolbar:

| Button | Shortcut | What It Does |
|--------|----------|--------------|
| **Accept** | `Ctrl+Enter` | Applies the LLM's changes |
| **Reject** | `Escape` | Throws them away, restores your original |

The diff is color-coded:
- **Green** = new code added
- **Yellow** = code changed
- **Red** = code removed

The editor is read-only while showing the diff so you can't accidentally edit during review.

---

## Features

### Multi-Backend LLM Support

| Backend | Description | Best For |
|---------|-------------|----------|
| **Ollama** | Easy local model management | Quick setup, many models |
| **GGUF** | llama.cpp quantized models | CPU inference, low VRAM |
| **MLX** | Apple Silicon optimized | M1/M2/M3/M4 Macs (default on macOS) |
| **vLLM** | High-throughput inference | NVIDIA GPUs, production |
| **Transformers** | HuggingFace models | Latest models (default on Linux) |
| **Blackwell** | NVIDIA Blackwell/DGX | Cutting-edge NVIDIA hardware |
| **Claude** | Anthropic API | Best cloud coding model |
| **OpenAI** | OpenAI API (GPT-4/5) | Cloud fallback |

**Platform-aware defaults**: MLX is auto-selected on macOS (Apple Silicon), Transformers on Linux. Falls back to Claude if neither is available.

### SEARCH/REPLACE Block Editing

When fixing code, the LLM can return only the parts that changed using a structured format:

```
<<<<<<< SEARCH
exact lines from the original code
=======
replacement lines
>>>>>>> REPLACE
```

CodeRunner automatically parses these blocks and applies them to your file — you never have to manually find and replace. If an exact match fails, fuzzy matching kicks in. This is what makes the fix loop fast: the LLM only sends a few lines instead of regenerating 500+ lines.

### Chat Edit Sync

You can **cut and paste** directly in the chat display to edit conversation history. When you click **Send**, CodeRunner rebuilds the message list from the visible chat text. This means you can:

- Delete old messages by cutting them out
- Edit previous prompts before resending
- Clean up the context the LLM sees

### Three-Window Layout

| Window | Purpose |
|--------|---------|
| **Chat** | Your conversation with the LLM. Editable — changes affect what the LLM sees. |
| **System Messages** | Status info for the user: file creation, code analysis, browser diagnostics, fix mode status. |
| **Debug Console** | Runtime errors only — stderr/stdout from your generated code. Clickable line numbers jump to the IDE. |

### Token Counters & Speed Metrics

The chat header shows real-time stats for every message:

```
00:42   In:1,234 Out:567 | Total In:5,678 Out:2,345  (42.3 tok/s)
```

- **In / Out**: Input and output tokens for the last message
- **Total In / Out**: Running session totals (reset on `/restart`)
- **tok/s**: Output generation speed

### Integrated Code Editor (IDE Window)

- Syntax highlighting (Python, JavaScript, HTML)
- Line numbers with click-to-navigate
- Find & Replace (`Ctrl+F`)
- Inline diff view with Accept/Reject
- Run & Fix (`F6`) — the main workflow button
- LLM Fix button — also available in the chat window
- Welcome message on first open with quick-start instructions
- F1 opens beginner-focused help

### Smart Debugging

- **Clickable error lines** in the debug console — click to jump to the line
- **Browser error capture** — catches JavaScript errors from HTML games via a local error server
- **Auto-fix pipeline** — Run & Fix detects errors, sends them to LLM, shows the diff, all in one keystroke
- **Code analysis** — automatic structure analysis (imports, classes, functions, patterns) shown in system messages when code is loaded

### Visual Feedback

- Run & Fix button flashes orange while running
- Status label shows bold red **"Fixing..."** when the LLM is working
- Accept button flashes green when a diff arrives
- IDE window comes to front automatically when the diff is ready

### Game Presets

15 built-in game prompts for one-shot generation:

Space Invaders, Asteroids, Breakout, Pac-Man, Frogger, Centipede, Defender, Flappy Birds, Doom-Style FPS, Mario Kart, Mr. Do!, Minecraft, Super Mario Bros, Missile Command, Q*bert

Select from the dropdown and hit Send.

### Defaults

| Setting | Default | Notes |
|---------|---------|-------|
| Temperature | 0.1 | Low for deterministic one-shot generation on local LLMs |
| Top-p | 0.5 | Nucleus sampling cutoff |
| Max tokens | 16,000 | Enough for full game generation |
| Backend | MLX (macOS) / Transformers (Linux) | Auto-detected based on platform |

---

## Keyboard Shortcuts

| Key | Action |
|-----|--------|
| `F1` | Open help |
| `F5` | Run code |
| `F6` | Run & Fix (run, catch errors, auto-fix) |
| `Ctrl+Enter` | Accept LLM's changes |
| `Escape` | Reject LLM's changes |
| `Ctrl+S` | Save file |
| `Ctrl+N` | New file |
| `Ctrl+O` | Open file |
| `Ctrl+F` | Find text |
| `Ctrl+G` | Go to line |

Mac users: `Cmd` works in place of `Ctrl` for all shortcuts.

---

## Architecture

```
+----------------------------------------------------------------+
|  IDE Toolbar:                                                   |
|  [New][Save][Load][Find] | [Run][Run&Fix][Timed] |             |
|  [LLM Fix] | [Accept][Reject]             Status: Ready        |
+------------------+-----------------+---------------------------+
|   Chat Panel     |   Code Editor   |  System / Debug Console    |
|                  |                 |                            |
|  [User Input]    |  [Your Code]    |  System: status, analysis  |
|  [AI Response]   |  [Diff View]    |  Debug: runtime errors     |
|  [Send][LLM Fix] |  [Accept/Rej]   |  [Click to Navigate]       |
+------------------+-----------------+---------------------------+
         |                 |                    |
         v                 v                    v
+----------------------------------------------------------------+
|                    Backend Manager                              |
+---------+---------+---------+---------+---------+--------------+
| Ollama  |  GGUF   |   MLX   |  vLLM   | Claude  |  OpenAI     |
+---------+---------+---------+---------+---------+--------------+
```

### Chat Header

```
[Move to IDE] [Open IDE]  Chat History:  00:42  In:1,234 Out:567 | Total In:5,678 Out:2,345 (42.3 tok/s)
```

### Chat Input Bar

```
[User Input Box]  [Send] [Send + Search] [LLM Fix]
```

### Context Menu (Right-Click in Editor)

- **Code**: Run, Run & Fix, Fix with LLM
- **Review**: Accept Changes, Reject Changes
- **Edit**: Undo, Redo, Cut, Copy, Paste
- **File**: New, Open, Save, Save As, Move to Chat
- **Search**: Find Text, Go to Line
- **View**: Refresh Highlighting, Update Line Numbers
- **Quick Actions**: Run & Fix, Accept, Reject, Save

---

## System Prompts

| Mode | System Message |
|------|----------------|
| **Python Programmer** | Expert Python/PyGame programmer. Include all features, console debugging. |
| **HTML Programmer** | Expert HTML5/Canvas programmer. No external files, Canvas drawing only. |
| **Fix Mode** (automatic) | Expert debugger. Return ONLY fixed functions in a single code block. Never return the entire program. |

Fix mode uses its own system message that replaces the generation-focused one. This prevents contradictory instructions ("include all features" vs "only return changed functions").

Custom prompts can be edited directly in the UI via the system prompt editor.

---

## Configuration

### API Keys

Store in `.env` (never committed to git):

```
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
```

Or place them in `anthropic_key.txt` / `openai_key.txt` next to the script.

### Recommended Models for Coding

| Model | Size | Backend | Notes |
|-------|------|---------|-------|
| `qwen2.5-coder:32b` | 32B | Ollama | Excellent for code |
| `deepseek-coder-v2` | 16B | Ollama | Great reasoning |
| `codellama:34b` | 34B | GGUF | Classic coding model |
| `Qwen/Qwen3-Coder` | — | MLX/Ollama | Latest Qwen coder |

---

## Installation Details

### Core (required)

```bash
pip install ollama requests pillow python-dotenv
```

### Local Backends (install as needed)

```bash
pip install llama-cpp-python       # GGUF models
pip install mlx mlx-lm             # Apple Silicon (macOS only)
pip install vllm                   # NVIDIA GPU acceleration
pip install transformers torch accelerate  # HuggingFace models
```

### Code Intelligence (optional)

```bash
pip install jedi pygments flake8 black
pip install mypy bandit radon      # Advanced analysis
```

---

## Troubleshooting

### "Model not loading"

- **Ollama**: Ensure `ollama serve` is running
- **GGUF**: Check you have enough RAM (model size x 1.2)
- **MLX**: Requires Apple Silicon Mac

### "CUDA out of memory"

- Use a smaller quantization (Q4 instead of Q8)
- Reduce `max_tokens` in settings
- Try CPU inference with GGUF

### "Tkinter not found"

```bash
# macOS
brew install python-tk

# Ubuntu/Debian
sudo apt-get install python3-tk

# Windows — included with Python installer
```

---

## File Structure

```
Code_Runner/
  CodeRunner_IDE_clean.py    # Main application (single file)
  requirements.txt           # Python dependencies
  config.json                # Saved configuration
  .env.example               # API key template
  .env                       # Your API keys (not committed)
  .gitignore                 # Git ignore rules
  Generated_games/           # Games generated by CodeRunner
  README.md                  # This file
```

---

## Contributing

Contributions welcome! Areas of interest:

- Additional backend support
- UI/UX improvements
- New system prompts for specific domains
- Documentation and examples

## License

MIT License — use freely for personal and commercial projects.

## Author

**Jonathan M. Rothberg** — Inventor of fast DNA sequencing, ultrasound-on-a-chip (Butterfly), portable MRI (Hyperfine), and next-gen protein sequencing (Quantum-Si).

- Twitter/X: [@jmrothberg](https://twitter.com/jmrothberg)
- GitHub: [jmrothberg](https://github.com/jmrothberg)

---

*Built with the belief that local AI should be as powerful as cloud AI.*
